"""This script executes the BLIP2 model within a modal container.

The user can interact with the BLIP2 model by asking the questions
from openadapt/vision/questions.txt about the images in openadapt/vision/images.
To run this script, make sure you have created a modal token by running the following:

    $ modal token new

Usage:

    $ modal run openadapt/vision/blip2.py

"""

from modal import Image, Stub, method
from PIL import Image as PILImage

BASE_MODEL = "Salesforce/blip2-flan-t5-xxl"
IMAGES_PATH = "openadapt/vision/images"
QUESTIONS_PATH = "openadapt/vision/questions.txt"

TIMEOUT = 18000


def download_models() -> None:
    """Download the BLIP2 model from Hugging Face's Transformers library."""
    from transformers import Blip2ForConditionalGeneration, Blip2Processor

    Blip2Processor.from_pretrained(BASE_MODEL)
    Blip2ForConditionalGeneration.from_pretrained(BASE_MODEL)


image = (
    Image.debian_slim(python_version="3.10")
    .pip_install("accelerate~=0.18.0", "transformers~=4.28.1", "torch~=2.0.0", "Pillow")
    .run_function(download_models)
)

stub = Stub(name="blip2_flan_t5_xxl", image=image)


@stub.cls(gpu="A100", timeout=TIMEOUT)
class Blip2Model:
    """This class facilitates interactions with the BLIP2 model.

    Sample usage where pil_image is an open PIL Image and question is a string:
    >>> model = Blip2Model()
    >>> model.generate.call(pil_image, question)
    """

    def __enter__(self) -> None:
        """Initialize the BLIP2 model and processor.

        Automatically runs upon initializing the Blip2Model class.
        """
        from transformers import Blip2ForConditionalGeneration, Blip2Processor
        import torch

        self.processor = Blip2Processor.from_pretrained(BASE_MODEL)

        self.model = Blip2ForConditionalGeneration.from_pretrained(
            BASE_MODEL, torch_dtype=torch.float16, device_map="auto"
        )
        self.device = "cuda"

    @method()
    def generate(
        self,
        image: PILImage,
        question: str,
    ) -> str:
        """Generate a completion for the given question about the given image.

        Use .call to call this function in the modal container.

        Args:
            image (PILImage): an image to pass to the model
            question (str): a question about the image

        Returns:
            str: the completion generated by the model
        """
        import torch

        inputs = self.processor(image, question, return_tensors="pt").to(
            self.device, torch.float16
        )
        out = self.model.generate(**inputs)
        answer = self.processor.decode(out[0], skip_special_tokens=True)
        return answer


@stub.local_entrypoint()
def main() -> None:
    """Entrypoint for the modal container."""
    import os

    from PIL import Image

    model = Blip2Model()

    with open(QUESTIONS_PATH, "r") as file:
        lines = file.readlines()
        questions = [line.strip() for line in lines]

    images = os.listdir(IMAGES_PATH)
    for image in images:
        print(f"Current image: {image}")
        image_path = os.path.join(IMAGES_PATH, image)
        pil_image = Image.open(image_path).convert("RGB")

        for question in questions:
            print(f"Question: {question}")
            response = model.generate.call(
                pil_image,
                question,
            )
            print(f"Answer: {response}")

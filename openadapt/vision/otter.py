"""This script executes the Otter model within a modal container.

The user can interact with the Otter model by asking the questions
from openadapt/vision/questions.txt about the images in openadapt/vision/images.

There are two available checkpoints, 9B and MPT7B. To change which one is being used,
modify the download_model function to use either BASE_MODEL_9B or BASE_MODEL_MPT7B.
To run this script, make sure you have created a modal token by running the following:

    $ modal token new

Usage:

    $ modal run openadapt/vision/otter.py

"""

from modal import Image, Stub, method
from PIL import Image as PILImage

BASE_MODEL_9B = "luodian/OTTER-9B-LA-InContext"
# uncomment the model below to use it in download_model
# BASE_MODEL_MPT7B = "luodian/OTTER-Image-MPT7B"
IMAGES_PATH = "openadapt/vision/images"
QUESTIONS_PATH = "openadapt/vision/questions.txt"

TIMEOUT = 18000
MAX_NEW_TOKENS = (512,)
NUM_BEAMS = (3,)
NO_REPEAT_NGRAM_SIZE = (3,)


def download_model() -> None:
    """This function downloads the necessary Otter models as a Hugging Face model.

    Change BASE_MODEL_9B to BASE_MODEL_MPT7B to try the MPT7B model.
    """
    import sys

    sys.path.append("/root/Otter")

    from otter.modeling_otter import OtterForConditionalGeneration

    OtterForConditionalGeneration.from_pretrained(BASE_MODEL_9B, device_map="auto")


image = (
    Image.debian_slim(python_version="3.9")
    .apt_install("git", "gcc", "build-essential", "cmake")
    .pip_install(
        "accelerate>=0.19.0",
        "braceexpand>=0.1.7",
        "einops>=0.6.1",
        "einops_exts>=0.0.4",
        "fastapi>=0.95.2",
        "gradio>=3.33.1",
        "horovod>=0.27.0",
        "huggingface_hub>=0.13.3",
        "importlib_metadata>=6.6.0",
        "inflection>=0.5.1",
        "markdown2>=2.4.8",
        "more_itertools>=9.1.0",
        "nltk>=3.8.1",
        "numpy>=1.23.5",
        "open_clip_torch>=2.16.0",
        "opencv_python_headless>=4.5.5.64",
        "peft",
        "Pillow>=9.5.0",
        "pycocoevalcap>=1.",
        "pycocotools>=2.0.6",
        "Requests>=2.31.0",
        "scipy>=1.10.1",
        "timm>=0.9.2",
        "tqdm>=4.65.0",
        "transformers==4.29.0",
        "uvicorn>=0.22.0",
        "webdataset>=0.2.48",
        "xformers>=0.0.20",
        "natsort>=8.4.0",
    )
    .run_commands(
        "git clone https://github.com/Luodian/Otter.git /root/Otter",
    )
    .run_function(download_model)
)

stub = Stub(name="otter_llama7b", image=image)

if stub.is_inside():
    import sys

    import transformers


@stub.cls(gpu="a100", timeout=TIMEOUT)
class OtterModel:
    """This class facilitates interactions with the Otter model.

    Sample usage where pil_image is an open PIL Image and question is a string:
    >>> model = OtterModel()
    >>> model.generate.call(pil_image, question)
    """

    def __enter__(self) -> None:
        """Initialize the Otter model and processor.

        Automatically runs upon initializing the OtterModel class.
        """
        sys.path.append("/root/Otter")

        from otter.modeling_otter import OtterForConditionalGeneration
        import torch

        load_bit = "bf16"
        precision = {}
        if load_bit == "bf16":
            precision["torch_dtype"] = torch.bfloat16
        elif load_bit == "fp16":
            precision["torch_dtype"] = torch.float16
        elif load_bit == "fp32":
            precision["torch_dtype"] = torch.float32
        self.model = OtterForConditionalGeneration.from_pretrained(
            "luodian/OTTER-9B-LA-InContext", device_map="sequential", **precision
        )
        self.model.text_tokenizer.padding_side = "left"
        self.tokenizer = self.model.text_tokenizer
        self.image_processor = transformers.CLIPImageProcessor()
        self.model.eval()

    def get_formatted_prompt(self, prompt: str) -> str:
        """Format the given prompt for interaction with the Otter model.

        Args:
            prompt (str): a prompt to be given to the model

        Returns:
            str: a formatted prompt that can now be given to the model
        """
        return f"<image>User: {prompt} GPT:<answer>"

    @method()
    def generate(self, image: PILImage, question: str) -> str:
        """Generate a completion for the given question about the given image.

        Use .call to call this function in the modal container.

        Args:
            image (PILImage): an image to pass to the model
            question (str): a question about the image

        Returns:
            str: the completion generated by the model
        """
        input_data = image

        vision_x = (
            self.image_processor.preprocess([input_data], return_tensors="pt")[
                "pixel_values"
            ]
            .unsqueeze(1)
            .unsqueeze(0)
        )

        lang_x = self.model.text_tokenizer(
            [
                self.get_formatted_prompt(question),
            ],
            return_tensors="pt",
        )

        model_dtype = next(self.model.parameters()).dtype

        vision_x = vision_x.to(dtype=model_dtype)
        lang_x_input_ids = lang_x["input_ids"]
        lang_x_attention_mask = lang_x["attention_mask"]

        bad_words_id = self.model.text_tokenizer(
            ["User:", "GPT1:", "GFT:", "GPT:"], add_special_tokens=False
        ).input_ids
        generated_text = self.model.generate(
            vision_x=vision_x.to(self.model.device),
            lang_x=lang_x_input_ids.to(self.model.device),
            attention_mask=lang_x_attention_mask.to(self.model.device),
            max_new_tokens=MAX_NEW_TOKENS,
            num_beams=NUM_BEAMS,
            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,
            bad_words_ids=bad_words_id,
        )
        parsed_output = (
            self.model.text_tokenizer.decode(generated_text[0])
            .split("<answer>")[-1]
            .lstrip()
            .rstrip()
            .split("<|endofchunk|>")[0]
            .lstrip()
            .rstrip()
            .lstrip('"')
            .rstrip('"')
        )
        return parsed_output


@stub.local_entrypoint()
def main() -> None:
    """Entrypoint for the modal container."""
    import os

    from PIL import Image

    model = OtterModel()

    with open(QUESTIONS_PATH, "r") as file:
        lines = file.readlines()
        questions = [line.strip() for line in lines]

    images = os.listdir(IMAGES_PATH)
    for image in images:
        print(f"Current image: {image}")
        image_path = os.path.join(IMAGES_PATH, image)
        pil_image = Image.open(image_path).convert("RGB")

        for question in questions:
            print(f"Question: {question}")
            response = model.generate.call(
                pil_image,
                question,
            )
            print(f"Answer: {response}")
